is it a good practice to use K-Fold cross validation instead of training, validation and test set? i have a Dataset of 5000 samples for a regression problem, now with this number of samples can i and is it better to use K-Fold cross validation instead of the validation set as an alternative?if not i've been splitting data as 70% 15% 15%, but it doesn't seem the distribution of splits are exactly the same, i mean depending on the split i would get very different predictions, for example in split i tune the model with validation set and get 0.75 $R^2$ for validation set and then get a $R^2$ if 0.88 for test set, and in another instance its the exact opposite.if i do a few random splits and find a few splits that the model structure is not very different in each split, can i just use one of these splits? wouldn't that defeat the purpose of the randomness of splits? out of these splits i chose two random splits, now after training, tuning and all the other steps, for one of the splits i would get the following $R^2$ for train, dev and test set: 0.91, 0.92, 0.90 and for the other one i would get these:  0.93, 0.88, 0.85. can i use the first one that gave me the best result? is it valid? i mean depending on the split it seems my final test score would be totally different.