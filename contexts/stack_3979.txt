On masked multi-head attention and layer normalization in transformer model I came to read Attention is All you Need by Vaswani. There two questions came up to me:1. How is it possible to mask out illegal connections in decoder multi-head attention?It says by setting something to negative infinity, they could prevent leftward information flow. Are they masking out attention weights or the hidden states from previous layer?2. Is it alright to set some arbitrary max_length for layer normalization?Let's say I set max_len 200. Whenever a sentence shorter than this comes in, LayerNorm will do whitening(i.e. subtract mean and divide by standard deviation) and linear mapping. The problem, I think is zero padding greatly affects whitening process. If a batch is composed of short sentences, like 10 or 20, then zero paddings are almost 80% of whole batch, which causes whitening makes data more close to zero-norm. Is this orthodox method? or is there any other practice?