Statistically prove minimum number of classifiers to classify 12 datasets I'm currently developing neural networks to classify medical images. One dataset contains 2 classes of images (Images which can be used and cannot used). So my model will check whether given image can be used or not.This is a binary classification.Likewise, I have 12 datasets (Each has 2 class images) to classify. All these 12 datasets are similar but not identical. So I thought to make 12 neural networks to perform classifications.But one model might be 2GB in size and having 12 models will arise a storage problem. So I need to find out minimum number of models should I train to evaluate all these 12 datasets.What I didI already trained one neural network for one dataset and use that model to evaluate rest (11 datasets). So, I got 95% of prediction accuracy for 5 datasets and 6 datasets with lower than 90% accuracy.Since I'm dealing with medical images, accuracy of classification should be high like 98%. So I'd like to know the statistical approach to find out the best possible minimum number of models for this? Can I use methods like ROC ?