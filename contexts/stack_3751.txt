Why limiting the weights to not grow to big numbers help to avoid overfitting? I understand that in order to avoid overfitting we need to reduce the complexity of the network. Or, in other words we can reduce the degree of polynomial. L1-norm does exactly this - reduces the degree of polynomial by making the solution sparse.In L2-norm case the weights don't become zero but small. Thus, the degree of polynomial does not decrease, meaning the network does not become less complex, meaning it did not solve the overfitting. So, I don't understand how limiting the weights to not grow to big numbers help to avoid overfitting.