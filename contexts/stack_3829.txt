Cross Validation benefits LOOCV v.s K-Fold I understand Cross Validation is used to parameter tuning and finding the machine learning model that will generalize well on the test data/Leave one out cross validation: One data point is left out (validation) and trained on rest of the data points. This is done iteratively and then results are averaged out. But how do we find the best model when the results are average out ? K-Fold we say K = 5 , 4 fold data is trained on and 1 fold data is validated and the results again are average out. Again how do we know which is the best model here ? Or do we choose a hyper parameter and do K-Fold here to see best hyper parameter in the model ? Lastly, what is the difference between grid search CV and K-Fold ?