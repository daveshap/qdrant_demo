Is there an error that considers both the absolute error and the standard deviation in a prediction? Say I'm building some kind of regression and I want to measure how good it is.Of course I want to measure the absolute error between the prediction and the real outcome, and if that was all I cared about, I could use Mean Squared Error (MSE), but my regression also gives me information about the standard deviation expected from its predictions at each point and I would like to incorporate this standard deviation into the \"goodness\" of my regression.Specifically, what I want is to penalize the errors more when the model estimates a small standard deviation about its predictions, but also penalize large standard deviations per se.Is the likelihood all I need? I know that maximizing the log likelihood, for example, already takes into account the standard deviation, but I'm not sure that it takes into account the absolute error, or at least not explicitly.