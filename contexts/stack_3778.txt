Why fewer feature classification models can perform better? I have seen articles on the \u201ccurse of dimensionality\u201d and why reducing the number of features can help with overfitting, but imagine we are interested in cases with significantly less features. Why is it that a classification model with fewer features can perform better than one with more features when the feature set is already small (for the sake of example, let's say 10 features)?