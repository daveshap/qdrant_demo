how to stabalize GAN learning I implemented a generative adversarial network. The generator and the discriminator both seem to learn independently. I put them together in the following manner : pre-train the discriminator to distinguish between the untrained    generator output and real output samples.I then iteratively  get half real data samples, half samples from the generator train the discriminator  train the entire GAN ( with the discriminator turned static ) by feeding in random numbers and telling the GAN that the output is all of the correct class. At first, the loss of the entire GAN is nearly 1 ( 100 % error ) and after a few iterations, it reaches about .2 or less at which point the discriminator and the GAN start to fight off so that one has a high loss while the other has a low loss. Then at some point after 50-100 iterations the GAN has a very high loss (0.99998) and the discriminator has very low loss ( 1e-5). The GAN makes some small comebacks but in general, the remainder of the training looks like that. Am I not waiting long enough for the GAN to beat the discriminator? Should I also pre-train the generator? Is there something else critical I am missing like needing the generator to have lot more parameters than the discriminator or learning rate differences...etc? Should I allow the GAN to learn a few iterations for every iteration that the discriminator gets?   Any knowhow/ experience you can share is welcome. Thank you