Intuition behind word vector representations How is it possible for a vector space to represent words so that it is coincident to our intuition of words? What does the orthogonality concept in such a space mean precisely?I think we can present so many vector models. However, none of them is coincident with our semantic understanding and our intuition of words or other linguistic terms we routinely use. If we simply present the vector space by assigning a dimension to each new word, I simply can ask what orthogonal or parallel words mean or what the distance between words exactly are.