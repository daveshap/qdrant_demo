Doesn't the non-Gaussian source assumption of ICA render it practically useless? Gaussian distributions appear everywhere in nature, indeed this was largely the justification for most classical methods' reliance on assumption of normality.ICA assumes non-Gaussian sources, indeed it requires this assumption to hold for the derivation of components via Central Limit Theorem.But what then is the value of a method that only works with assumptions contrary to the known reality? Note that unlike the many previous questions I am not asking why or how ICA requires non-Gaussianity. I understand and accept that ICA requires this. What I don't understand is how this doesn't destroy the perceived usefulness of ICA.What if one wished to separate a mixture of Gaussian sources? What if one didn't know the distributions of the sources (as is typical), then it would seem the 'default assumption' should be for rather than against their Gaussianity - in this case a method that can only give non-Gaussian answers is surely misleading at best?Are scientists really just 'flip-flopping' on how Gaussian they assume the world to be when they use ICA versus classical methods?