I took a training course on harassment. It seems there is a gender bias, why?
I took a training in harrassment in the workplace. It seemed to me that in the training, the woman was always the victim. So much that a few male staff member said something about it. I work in the mental health field and I do not believe that portrayal is accurate. I work with an almost 90% female population and have seen and heard some women be  unprofessional as well. Much more than then the men I've worked with. However, in the training it just made it seem that men are always the aggressor. Isn't this a type of indoctrination, making it seem women are always the victim?