Tossing coin and classical ML estimate I'm reading Bishop's Pattern recognition and came across with the next on the p.23:<blockquote>  Suppose, for instance, that a fair-looking coin is tossed three times  and lands heads each time. A classical maximum likelihood estimate of  the probability of landing heads would give 1, implying that all  future tosses will land heads! By contrast, a Bayesian approach with  any reasonable prior will lead to a much less extreme conclusion.</blockquote>Could you explain please, why it is so?