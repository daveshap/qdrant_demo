Convex optimization: Is gradient descent faster if a regularizer is added? I am not sure if this is a true statement or not but there appears to be an intuition around this among experts in the field that I do not quite understand.The idea is: Given a convex optimization problem that will be solved using gradient descent, it is sometimes advantageous to add an extra term to the objective function (the regularizer). On Wikipedia, it suggests that regularizers are good to avoid overfitting but in our case, the problem is simply finding an optimal $x^*$ that minimizes a convex function $f(x)$ so this doesn't seem to apply.So in what sense is adding a regularizer better for convex optimization? How can one prove, for instance, that convergence is faster if we add this term? I realize this is perhaps a basic and broad question so references are fine too, if an answer is too big to fit in the scope of this site.