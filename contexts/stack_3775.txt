NN works well on preproccessed data, but results are poor after de-normalizing - predicting non-normalized prediction values using NN? So I'm trying to build a neural network to fit an approximating function to a data set.The data set consists of (input,output) paris where the input features are discrete numbers [0,1,2,...,1027].While the output is a single continous number between [0, 1027).Now, If in preprocessing I normalize both my input features and outputs by dividing by the max value in the data set, my neural network which consists of a single hidden layer (relu) and a regression layer in the output learns well and I'm getting my MSE to drop to a nice level and I can plot and see that the NN fits well both the train set and the test set.The problem is, when I de-normalize back the estimations to their [0,1027) range, the erros get multiplied by the max value (1027) and the overall results are now very poor.Any tips on dealing with this kind of problem? how can I preform pre proccesing so my NN learn well, but not hurting performance when de-normalizing.Thanks.