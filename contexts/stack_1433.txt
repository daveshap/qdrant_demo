Rated vs. actual power usage of a linear fluorescent lamp After having some power issues, I've decided to investigate the largest power drains in our company, and I've arrived at a realization that 128 lights adds up to a lot of power (and that's not even all of them!).Overall, I'm looking at the feisability of replacing all lamps with LEDs, but I'd like to avoid surprises of actual power usage being significantly higher than rated. As I started calculating the power usage, I've realized that there is a fairly large difference between the actual power draw and rated power.The measured current at the grid connection was 0.31667 A / lamp - lowest result was used as \"currently most efficient\though there might have been a busted lamp among them. Measurements were made in different areas for reliability and I made sure all other power users were accounted for. In this case, all lamps are linear fluorescent lamps, T8 58W 150cm, using starters.Using the \"standard\" 220 V, I get the following result:<blockquote>  0.31667 A * 220 V = 69.6674 W</blockquote>Using the calculated 233.33 V from the 35 kW, 3 x 50 A grid connection, I get this instead:<blockquote>  0.31667 A * 233.33 V = 73.8886 W </blockquote>However, both are a far cry from the lamps' rated 58 W<blockquote>  0.31667 A * 183.1560 V = 58 W</blockquote>Am I calculating something wrong? Is the discrepancy due to lamp deterioration or starters? Is there something else obvious that I'm missing?Is this discrepancy also something I should worry with the planned LED replacements? Considering current lamps are 58 W and LED replacements would be 24 W.I'm no expert and although I'm well aware that A * V = W isn't how it always works, I presumed that it should be accurate enough for lamps.