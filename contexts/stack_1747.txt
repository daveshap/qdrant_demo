How do we compute power from current and voltage samples? Can we quantify its accuracy? I have a datalogger that periodically samples current values by measuring the voltage across a power resistor connected in series with a battery and sensor (the load). The datalogger also samples the voltage across the sensor. Let's call the current samples i[n] and voltage samples v[n]. Sampling of both signals is controlled programmatically (i.e., I specify in code how often to sample each parameter).  p(t)=i(t)v(t) and if we had measured the current and voltage signals continuously (as analog signals and not sampling, as I'd done), we could have built a circuit that \"continuously multiplied\" i(t) and v(t). p(t) would have been a continuous value. Instead, we have the discretized the signal by sampling it and so we could only compute p[n] as i[n]*v[n]. How representative is p[n] of the actual power being drawn, p(t)? Why might someone (say an engineer who's in charge of designing the power subsystem) be interested in sampling p[n] at a higher frequency? Also, what can we say about p[n] if the samples weren't simultaneous (i.e., i[1] was measured 1.5 seconds from the start and v[1] was measured 1.525 seconds from the start)?