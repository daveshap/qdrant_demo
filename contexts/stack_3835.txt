Why does $[0,1]$ scaling dramatically increase training time for feed forward ANN (1 hidden layer)? I am using an ANN for classification. My covariates are the relative lagged returns for gold, SPX and Oil. When i do not scale my inputs between 0 and 1 I achieve a fast training time. however after scaling my training time increases dramatically, sometimes 40 fold.My accuracy is also better when inputs are not scaled. I cant seem to find any reason for this online. Has anyone got any ideas?