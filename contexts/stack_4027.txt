What if cross-validation fails to prevent overfitting I'm training a random forest model with AUC as performance metric. I've splitted my data to train set (70%) and test set (30%) and performed cross-validation on train set to tune the hyperparameters. As for now, I've ended up with a model that has ~0.95 AUC on the training data, ~0.85 AUC in cross-validation process and ~0.84 AUC on the test data. It seems to me that cross-validation failed in that case and although it estimate test error pretty nice, my model is overfitting. I know that one way to prevent overfitting is to get more data, but it's usually not possible (as in this case). In case of linear or logistic regression I could remove some features as well, but I think random forests naturally ignores features that are irrelevant. Why, despite using cross-validation, my model overfits? What can be done to resolve that issue?