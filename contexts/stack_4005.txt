Is a polynomial kernel ridge regression really equivalent to performing a linear regression on those expanded features? Say we have a dataset, X, which is Nx2 where N is the number of examples and 2 is the number of dimensions \"features\". If we were to run a kernel ridge regression (or SVM or whatever) on these features using a polynomial kernel of degree 2, it is my understanding that this would be equivalent to mapping your 2 dimensions to a feature space of all pairwise products and squares between the two dimensions, along with appropriate coefficients, and then performing linear regression in that space.My question is: is what I said above true? My confusion lies in the fact that the feature mapping that the literature says to use is some fixed mapping x1,x2 -> 1 + x1^2 + x2^2 + sqrt(2) * x1x2, so the relative weights for each of those terms is fixed. However, if we were to run a linear regression based on those features, our model is free to learn any relative weighting that's optimal. In particular, it seems like the latter approach has more degrees of freedom (I'm not 100% clear on how to describe this confusion).