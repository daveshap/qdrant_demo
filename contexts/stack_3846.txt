Feedforward Nets, RNNs, and LSTMs Theory It is my understanding that feed-forward neural nets learn by backpropegating the error from comparing an outcome to the ground truth. How is this process not 'recurrent', as in recurrent nets, data is passed backwards through the model right? What is the difference between feed-forward nets and recurrent nets?In addition, how do LSTM units play a role in recurrent nets? It is my understanding that LSTM cells 'remember' values over arbitrary time intervals, and the gates regulate the flow of data. But what exactly are these cells? Are they separate from the nodes of a recurrent net? And how do gates regulate the flow of data? Does this mean that some data skips passing over certain nodes?