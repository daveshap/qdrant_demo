What show am I remembering about aliens taking over Earth? I think in this show aliens have taken over the earth and essentially humans are working with/for them - but although things seem better the aliens are really farming people for food. I remember three specific scenes/plot lines and I believe both are from the same show: there is some sort of maternity/fertility clinic and video billboards urge women to come to it; there is a teen or young woman who is pregnant and some character close to her tells her not to go to that clinic; there is a woman (doctor? scientist?) maybe working at this clinic and she discovers the doctor is involved with giving the babies to the aliens. Next scene - a lower level area (maybe in clinic) is discovered where people are being processed - hanging on hooks like cattle; there is a fight there as good guys try to save people and stop this. Last scene, people are sent (voluntarily?) to what is supposed to be a good place to live and work, maybe sold as sort of a farm or something, and good guys discover it the people who go there are killed; I think they have to show the facility to someone who does not believe this is happening.I can't remember any actors, and my googling efforts have run out of key words etc. It is NOT \"V\" - I think I saw this within the past few years, should be something made between 2000 and 2017. I am sure I watched it as home, and it was more likely a series or miniseries than a feature film. Maybe a short lived series?