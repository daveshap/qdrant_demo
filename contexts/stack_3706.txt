Test if performance difference of two image segmentation algorithms is statistically significant I have been working with a dataset of 240 medical images of brain tumours.The dataset is not particularly large but it is heavy, meaning each image is large (in memory) and takes a long time to process.I want to compare the performance of two segmentation algorithms. These algorithms take a long time to train (2 days already in a GPU) making it painful to perform large experiments such as 10-fold cross-validation.I was wondering what is the best way to produce some measure of statistical significance for the difference in performance whilst conducting the fewest runs possible.What I am thinking:T-tests always recommend using at least 30 trials (n=30).Since my performance metric per case is the percentage of volume overlap (not misclassified or not like in pure classification) can I consider that each individual case in the holdout set is a trial? This would greatly simplify things since it would only require having a holdout set of around 30 cases. Is this valid or statistical nonsense?